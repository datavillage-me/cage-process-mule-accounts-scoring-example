"""
Example of a confidential workload processing mule accounts data from 3 financial institutions and sending the resulting score to the requesting financial institution.
The confidential workload handles 2 events: one to perform scoring example, and one to trigger the data holders' data quality checks.
"""

import logging
import time
import os
import json
import duckdb
from datetime import datetime
import base64
import shutil

#cryptodome
from Crypto.Cipher import PKCS1_OAEP
from Crypto.PublicKey import RSA

from dv_utils import default_settings, ContractManager,audit_log,LogLevel

logger = logging.getLogger(__name__)

# let the log go to stdout, as it will be captured by the cage operator
logging.basicConfig(
    level=default_settings.log_level,
    format="%(asctime)s - %(levelname)s - %(message)s",
)

keys_input_dir = "/resources/data"
#keys_input_dir = "demo-keys"

# define an event processing function
def event_processor(evt: dict):
    """
    Process an incoming event
    Exception raised by this function are handled by the default event listener and reported in the logs.
    """
    
    logger.info(f"Processing event {evt}")

    # dispatch events according to their type
    evt_type =evt.get("type", "")

    if evt_type == "CHECK_DATA_QUALITY":
        # use the CHECK_DATA_QUALITY event processor dedicated function
        logger.info(f"Use the check data quality event processor")
        check_data_quality_contracts_event_processor(evt)
    if(evt_type == "SCORE"):
        # use the SCORE  event processor dedicated function
        logger.info(f"Use the scoring event processor")
        process_score_event(evt)
    else:
        generic_event_processor(evt)


def generic_event_processor(evt: dict):
    # push an audit log to reccord for an event that is not understood
    logger.info(f"Received an unhandled event {evt}")

def check_data_quality_contracts_event_processor(evt: dict):
    #audit logs are generated by the dv_utils sdk
    try:
        contractManager=ContractManager()
        contractManager.check_contracts_for_collaboration_space(default_settings.collaboration_space_id)
    except Exception as e:
        logger.error(e)

def process_score_event(evt: dict):
    """
    Score mule accounts 
     """

    logger.info(f"---------------------------------------------------------")
    logger.info(f"|                    START PROCESSING                   |")
    logger.info(f"|                                                       |")
    start_time = time.time()
    logger.info(f"|    Start time:  {start_time} secs               |")
    logger.info(f"|                                                       |")
    audit_log(f"Start processing event: {evt.get('type', '')}.",LogLevel.INFO)
    
    #load parameters
    accountsList= evt.get("accounts", "")

    #load private keys
    with open (keys_input_dir+"/key.pem", "rb") as prv_file:
        privateKey=prv_file.read()
    cipher = PKCS1_OAEP.new(RSA.importKey(privateKey))
    try:
        collaboration_space_id=default_settings.collaboration_space_id
        logger.info(f"| 1. Get data contracts                                 |")
        logger.info(f"|                                                       |")
        contractManager=ContractManager()
        data_contracts=contractManager.get_contracts_for_collaboration_space(collaboration_space_id)
        if data_contracts != None and len(data_contracts)>0:
            #Create in memory duckdb (encrypted memory on confidential computing)
            con = duckdb.connect(database=":memory:")
            #Add connector settings to duckdb con for all data contracts (3 data contracts in this example)
            con = data_contracts[0].connector.add_duck_db_connection(con)
            con = data_contracts[1].connector.add_duck_db_connection(con)
            con = data_contracts[2].connector.add_duck_db_connection(con)

            #check if data exists in memory
            res=con.sql("SHOW ALL TABLES; ")
            if len(res)==0:
                logger.info(f"| 2. Read data from data holders                        |")
                #Create duckdb in memory db for optimisation
                query=f"SELECT * FROM {data_contracts[0].connector.get_duckdb_source()}"
                res=con.sql("CREATE OR REPLACE TABLE mule_accounts_1 AS "+query) 
                audit_log(f"Read data from: {data_contracts[0].data_descriptor_id}.",LogLevel.INFO)
                query=f"SELECT * FROM {data_contracts[1].connector.get_duckdb_source()}"
                res=con.sql("CREATE OR REPLACE TABLE mule_accounts_2 AS "+query) 
                audit_log(f"Read data from: {data_contracts[1].data_descriptor_id}.",LogLevel.INFO)
                query=f"SELECT * FROM {data_contracts[2].connector.get_duckdb_source()}"
                res=con.sql("CREATE OR REPLACE TABLE mule_accounts_3 AS "+query) 
                audit_log(f"Read data from: {data_contracts[2].data_descriptor_id}.",LogLevel.INFO)
            logger.info(f"|                                                       |")
            logger.info(f"| 2. Calculate scoring                                  |")
            output="" 
            for x in range(len(accountsList)):
                accountId=base64.b64decode(accountsList[x])
                accountId = cipher.decrypt(accountId).decode("utf-8")
                #accountId=accountsList[x]
                query=f"SELECT * FROM mule_accounts_1 WHERE account_id='{accountId}' UNION ALL SELECT * FROM mule_accounts_2 WHERE account_id='{accountId}' UNION ALL SELECT * FROM mule_accounts_3 WHERE account_id='{accountId}'"
                muleOutput=find_mule_account(query,con)
                if muleOutput!="" :
                    output=output+muleOutput+","
            if output!="" :
                output=output[:-1]
            collaborationOutput='''{
            "report_date": "'''+datetime.today().strftime('%Y-%m-%d %H:%M:%S')+'''",
            "accounts": [ '''+output+'''
            ]
            }
            '''
            logger.info(f"|                                                       |")
            logger.info(f"| 3. Send scoring report                                |")
            with open(default_settings.data_user_output_location+'/scoring-report.json', 'w', newline='') as file:
                file.write(collaborationOutput)
            logger.info(f"|                                                       |")
            execution_time=(time.time() - start_time)
            logger.info(f"|    Execution time:  {execution_time} secs           |")
            logger.info(f"|                                                       |")
            logger.info(f"--------------------------------------------------------")
        else:
            logger.error(f"No data contract available for collaboration_space_id: {collaboration_space_id}")
    except Exception as e:
        logger.error(e) 
   
def find_mule_account(query,con):
    df = con.sql(query).df()
    numberOfScoringParties=0
    combinedRiskScore=0
    redFlags=""
    accountHolderName=""
    account_id=""
    output=""
    for index, row in df.iterrows():
        account_id=row["account_id"]
        accountHolderName=accountHolderName+'"'+row["account_holder_name"]+'",'
        numberOfScoringParties=numberOfScoringParties+1
        redFlagsArray=json.loads(row["red_flag"])["red_flags"]
        for x in range(len(redFlagsArray)):
            redFlags=redFlags+str(redFlagsArray[x])+","
        combinedRiskScore=combinedRiskScore+row["risk_score"]
    combinedRiskScore=combinedRiskScore/3
    if account_id!= "":
        output='''
        {
                "account_id": "'''+account_id+'''",
                "account_holder_names": [
                    '''+accountHolderName[:-1]+'''
                ],
                "combined_risk_score": "'''+str(int(combinedRiskScore))+'''",
                "number_of_scoring_parties":"'''+str(numberOfScoringParties)+'''",
                "red_flags": [
                    '''+redFlags.replace("'","\"")[:-1]+'''
                ]
            }
        '''
    return output